{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14679992,"sourceType":"datasetVersion","datasetId":9378444}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:59:28.396439Z","iopub.execute_input":"2026-01-30T18:59:28.396930Z","iopub.status.idle":"2026-01-30T18:59:28.669674Z","shell.execute_reply.started":"2026-01-30T18:59:28.396903Z","shell.execute_reply":"2026-01-30T18:59:28.669075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:59:32.406751Z","iopub.execute_input":"2026-01-30T18:59:32.407179Z","iopub.status.idle":"2026-01-30T18:59:32.878912Z","shell.execute_reply.started":"2026-01-30T18:59:32.407124Z","shell.execute_reply":"2026-01-30T18:59:32.878195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:59:34.628066Z","iopub.execute_input":"2026-01-30T18:59:34.628874Z","iopub.status.idle":"2026-01-30T18:59:34.748055Z","shell.execute_reply.started":"2026-01-30T18:59:34.628838Z","shell.execute_reply":"2026-01-30T18:59:34.747288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q \\\n  transformers \\\n  datasets \\\n  peft \\\n  bitsandbytes \\\n  accelerate \\\n  pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:59:35.599948Z","iopub.execute_input":"2026-01-30T18:59:35.600262Z","iopub.status.idle":"2026-01-30T18:59:42.352855Z","shell.execute_reply.started":"2026-01-30T18:59:35.600231Z","shell.execute_reply":"2026-01-30T18:59:42.352177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:59:47.642111Z","iopub.execute_input":"2026-01-30T18:59:47.642954Z","iopub.status.idle":"2026-01-30T18:59:54.050904Z","shell.execute_reply.started":"2026-01-30T18:59:47.642922Z","shell.execute_reply":"2026-01-30T18:59:54.050317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:00:08.861832Z","iopub.execute_input":"2026-01-30T19:00:08.862349Z","iopub.status.idle":"2026-01-30T19:00:08.883030Z","shell.execute_reply.started":"2026-01-30T19:00:08.862318Z","shell.execute_reply":"2026-01-30T19:00:08.882203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counsel_df = pd.read_csv(\"/kaggle/input/empathyy/counselchat.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:00:27.589621Z","iopub.execute_input":"2026-01-30T19:00:27.589924Z","iopub.status.idle":"2026-01-30T19:00:27.725910Z","shell.execute_reply.started":"2026-01-30T19:00:27.589900Z","shell.execute_reply":"2026-01-30T19:00:27.725354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counsel_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:00:55.310431Z","iopub.execute_input":"2026-01-30T19:00:55.310752Z","iopub.status.idle":"2026-01-30T19:00:55.338814Z","shell.execute_reply.started":"2026-01-30T19:00:55.310724Z","shell.execute_reply":"2026-01-30T19:00:55.338081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emp_df = pd.read_parquet(\"/kaggle/input/empathyy/emp_dialogues.parquet\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:00:59.278520Z","iopub.execute_input":"2026-01-30T19:00:59.279324Z","iopub.status.idle":"2026-01-30T19:00:59.494359Z","shell.execute_reply.started":"2026-01-30T19:00:59.279293Z","shell.execute_reply":"2026-01-30T19:00:59.493747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emp_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:01.138756Z","iopub.execute_input":"2026-01-30T19:01:01.139058Z","iopub.status.idle":"2026-01-30T19:01:01.151948Z","shell.execute_reply.started":"2026-01-30T19:01:01.139030Z","shell.execute_reply":"2026-01-30T19:01:01.151359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"CounselChat rows:\", len(counsel_df))\nprint(\"Empathetic Dialogues rows:\", len(emp_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:26.486232Z","iopub.execute_input":"2026-01-30T19:01:26.486530Z","iopub.status.idle":"2026-01-30T19:01:26.490778Z","shell.execute_reply.started":"2026-01-30T19:01:26.486504Z","shell.execute_reply":"2026-01-30T19:01:26.490076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keep only what we need\ncounsel_clean = counsel_df[[\"questionText\", \"answerText\"]].dropna()\n\n# Rename to standard names\ncounsel_clean = counsel_clean.rename(columns={\n    \"questionText\": \"user\",\n    \"answerText\": \"assistant\"\n})\n\n# Trim whitespace\ncounsel_clean[\"user\"] = counsel_clean[\"user\"].str.strip()\ncounsel_clean[\"assistant\"] = counsel_clean[\"assistant\"].str.strip()\n\nprint(counsel_clean.head())\nprint(\"CounselChat cleaned rows:\", len(counsel_clean))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:28.478440Z","iopub.execute_input":"2026-01-30T19:01:28.478734Z","iopub.status.idle":"2026-01-30T19:01:28.496548Z","shell.execute_reply.started":"2026-01-30T19:01:28.478708Z","shell.execute_reply":"2026-01-30T19:01:28.495818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\ndef extract_pairs(row):\n    pairs = []\n    convo = row[\"conversations\"]\n\n    # Ensure it's parsed JSON\n    if isinstance(convo, str):\n        convo = json.loads(convo)\n\n    for i in range(len(convo) - 1):\n        if convo[i][\"role\"] == \"user\" and convo[i+1][\"role\"] == \"assistant\":\n            user_text = convo[i][\"content\"].strip()\n            assistant_text = convo[i+1][\"content\"].strip()\n\n            if len(user_text) > 5 and len(assistant_text) > 5:\n                pairs.append({\n                    \"user\": user_text,\n                    \"assistant\": assistant_text,\n                    \"emotion\": row[\"emotion\"],\n                    \"situation\": row[\"situation\"]\n                })\n    return pairs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:30.326602Z","iopub.execute_input":"2026-01-30T19:01:30.326860Z","iopub.status.idle":"2026-01-30T19:01:30.332269Z","shell.execute_reply.started":"2026-01-30T19:01:30.326840Z","shell.execute_reply":"2026-01-30T19:01:30.331476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emp_pairs = []\n\nfor _, row in emp_df.iterrows():\n    emp_pairs.extend(extract_pairs(row))\n\nemp_clean = pd.DataFrame(emp_pairs)\n\nprint(emp_clean.head())\nprint(\"Extracted Empathetic Dialogue pairs:\", len(emp_clean))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:34.625066Z","iopub.execute_input":"2026-01-30T19:01:34.625794Z","iopub.status.idle":"2026-01-30T19:01:35.518357Z","shell.execute_reply.started":"2026-01-30T19:01:34.625766Z","shell.execute_reply":"2026-01-30T19:01:35.517257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def trim_text(df, max_user=300, max_assistant=400):\n    df = df.copy()\n    df[\"user\"] = df[\"user\"].str.slice(0, max_user)\n    df[\"assistant\"] = df[\"assistant\"].str.slice(0, max_assistant)\n    return df\n\ncounsel_clean = trim_text(counsel_clean)\nemp_clean = trim_text(emp_clean)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:38.204024Z","iopub.execute_input":"2026-01-30T19:01:38.204601Z","iopub.status.idle":"2026-01-30T19:01:38.233732Z","shell.execute_reply.started":"2026-01-30T19:01:38.204569Z","shell.execute_reply":"2026-01-30T19:01:38.233199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emp_clean[\"emotion\"].value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:40.630482Z","iopub.execute_input":"2026-01-30T19:01:40.630773Z","iopub.status.idle":"2026-01-30T19:01:40.642943Z","shell.execute_reply.started":"2026-01-30T19:01:40.630748Z","shell.execute_reply":"2026-01-30T19:01:40.642099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_emotion_context(row):\n    return f\"The user feels {row['emotion']}. {row['user']}\"\n\nemp_clean[\"user\"] = emp_clean.apply(add_emotion_context, axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:43.324804Z","iopub.execute_input":"2026-01-30T19:01:43.325530Z","iopub.status.idle":"2026-01-30T19:01:43.557394Z","shell.execute_reply.started":"2026-01-30T19:01:43.325501Z","shell.execute_reply":"2026-01-30T19:01:43.556777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emp_sampled = emp_clean.sample(\n    n=10000,\n    random_state=42\n)\n\nprint(\"CounselChat:\", len(counsel_clean))\nprint(\"EmpDialogues sampled:\", len(emp_sampled))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:45.431470Z","iopub.execute_input":"2026-01-30T19:01:45.431937Z","iopub.status.idle":"2026-01-30T19:01:45.440578Z","shell.execute_reply.started":"2026-01-30T19:01:45.431909Z","shell.execute_reply":"2026-01-30T19:01:45.439752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_df = pd.concat(\n    [counsel_clean[[\"user\", \"assistant\"]],\n     emp_sampled[[\"user\", \"assistant\"]]],\n    ignore_index=True\n)\n\nprint(\"Total combined rows:\", len(combined_df))\ncombined_df.sample(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:46.600916Z","iopub.execute_input":"2026-01-30T19:01:46.601491Z","iopub.status.idle":"2026-01-30T19:01:46.614157Z","shell.execute_reply.started":"2026-01-30T19:01:46.601463Z","shell.execute_reply":"2026-01-30T19:01:46.613545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emp_sampled[\"emotion\"].value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:49.731798Z","iopub.execute_input":"2026-01-30T19:01:49.732292Z","iopub.status.idle":"2026-01-30T19:01:49.739271Z","shell.execute_reply.started":"2026-01-30T19:01:49.732264Z","shell.execute_reply":"2026-01-30T19:01:49.738685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def to_chat_format(row):\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": row[\"user\"]},\n            {\"role\": \"assistant\", \"content\": row[\"assistant\"]}\n        ]\n    }\n\nchat_data = combined_df.apply(to_chat_format, axis=1).tolist()\n\nprint(chat_data[0])\nprint(\"Total chat samples:\", len(chat_data))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:52.839667Z","iopub.execute_input":"2026-01-30T19:01:52.839951Z","iopub.status.idle":"2026-01-30T19:01:52.928546Z","shell.execute_reply.started":"2026-01-30T19:01:52.839926Z","shell.execute_reply":"2026-01-30T19:01:52.927788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\nhf_dataset = Dataset.from_list(chat_data)\nhf_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:55.070901Z","iopub.execute_input":"2026-01-30T19:01:55.071757Z","iopub.status.idle":"2026-01-30T19:01:55.149772Z","shell.execute_reply.started":"2026-01-30T19:01:55.071717Z","shell.execute_reply":"2026-01-30T19:01:55.149216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:01:59.656529Z","iopub.execute_input":"2026-01-30T19:01:59.656827Z","iopub.status.idle":"2026-01-30T19:02:25.537043Z","shell.execute_reply.started":"2026-01-30T19:01:59.656800Z","shell.execute_reply":"2026-01-30T19:02:25.535726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport numpy as np\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n\ndef count_tokens(example):\n    text = \"\"\n    for msg in example[\"messages\"]:\n        text += msg[\"content\"]\n    return {\"token_count\": len(tokenizer(text)[\"input_ids\"])}\n\ntoken_counts = hf_dataset.map(count_tokens)\ntokens = token_counts[\"token_count\"]\n\nprint(\"Max tokens:\", np.max(tokens))\nprint(\"Mean tokens:\", int(np.mean(tokens)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:02:42.927054Z","iopub.execute_input":"2026-01-30T19:02:42.927784Z","iopub.status.idle":"2026-01-30T19:02:50.250269Z","shell.execute_reply.started":"2026-01-30T19:02:42.927748Z","shell.execute_reply":"2026-01-30T19:02:50.249435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_id = \"google/gemma-2-2b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\nimport torch\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_4bit=True,\n    device_map={\"\": 0},\n    torch_dtype=torch.float16,\n)\n\n# ðŸ”´ CRITICAL FIXES FOR GEMMA-2\nmodel.config.use_cache = False                # required for training\nmodel.config.attn_implementation = \"eager\"    # ðŸ”´ disables FlashAttention\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:03:29.123474Z","iopub.execute_input":"2026-01-30T19:03:29.124280Z","iopub.status.idle":"2026-01-30T19:04:32.196299Z","shell.execute_reply.started":"2026-01-30T19:03:29.124248Z","shell.execute_reply":"2026-01-30T19:04:32.195722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:04:44.756774Z","iopub.execute_input":"2026-01-30T19:04:44.757820Z","iopub.status.idle":"2026-01-30T19:04:44.864172Z","shell.execute_reply.started":"2026-01-30T19:04:44.757786Z","shell.execute_reply":"2026-01-30T19:04:44.863427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def flatten_chat(example):\n    text = \"\"\n    for msg in example[\"messages\"]:\n        if msg[\"role\"] == \"user\":\n            text += f\"User: {msg['content']}\\n\"\n        else:\n            text += f\"Assistant: {msg['content']}\\n\"\n    return {\"text\": text}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:04:48.628448Z","iopub.execute_input":"2026-01-30T19:04:48.628769Z","iopub.status.idle":"2026-01-30T19:04:48.633205Z","shell.execute_reply.started":"2026-01-30T19:04:48.628740Z","shell.execute_reply":"2026-01-30T19:04:48.632446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = hf_dataset.map(\n    flatten_chat,\n    remove_columns=hf_dataset.column_names\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:04:52.155310Z","iopub.execute_input":"2026-01-30T19:04:52.155999Z","iopub.status.idle":"2026-01-30T19:04:52.743532Z","shell.execute_reply.started":"2026-01-30T19:04:52.155968Z","shell.execute_reply":"2026-01-30T19:04:52.742733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_fn(example):\n    return tokenizer(\n        example[\"text\"],\n        truncation=True,\n        max_length=512,\n        padding=False\n    )\n\ntokenized_dataset = train_dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:04:53.998578Z","iopub.execute_input":"2026-01-30T19:04:53.998876Z","iopub.status.idle":"2026-01-30T19:04:55.406759Z","shell.execute_reply.started":"2026-01-30T19:04:53.998848Z","shell.execute_reply":"2026-01-30T19:04:55.406172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:04:59.422178Z","iopub.execute_input":"2026-01-30T19:04:59.422974Z","iopub.status.idle":"2026-01-30T19:04:59.426549Z","shell.execute_reply.started":"2026-01-30T19:04:59.422932Z","shell.execute_reply":"2026-01-30T19:04:59.425656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./gemma-empathy\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    num_train_epochs=2,\n    logging_steps=50,\n    save_steps=200,          # ðŸ”´ safer\n    save_total_limit=3,\n    report_to=\"none\",\n    optim=\"adamw_torch\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:05:01.283496Z","iopub.execute_input":"2026-01-30T19:05:01.283817Z","iopub.status.idle":"2026-01-30T19:05:01.324294Z","shell.execute_reply.started":"2026-01-30T19:05:01.283791Z","shell.execute_reply":"2026-01-30T19:05:01.323637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:05:03.845692Z","iopub.execute_input":"2026-01-30T19:05:03.846214Z","iopub.status.idle":"2026-01-30T19:05:03.862540Z","shell.execute_reply.started":"2026-01-30T19:05:03.846183Z","shell.execute_reply":"2026-01-30T19:05:03.861989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T19:05:06.445413Z","iopub.execute_input":"2026-01-30T19:05:06.445754Z","iopub.status.idle":"2026-01-30T21:37:48.717191Z","shell.execute_reply.started":"2026-01-30T19:05:06.445724Z","shell.execute_reply":"2026-01-30T21:37:48.716501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/gemma-empathy-lora\")\ntokenizer.save_pretrained(\"/kaggle/working/gemma-empathy-lora\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T21:43:27.174131Z","iopub.execute_input":"2026-01-30T21:43:27.174479Z","iopub.status.idle":"2026-01-30T21:43:27.938616Z","shell.execute_reply.started":"2026-01-30T21:43:27.174451Z","shell.execute_reply":"2026-01-30T21:43:27.937778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=200,\n    temperature=0.7,\n    do_sample=True\n)\n\nprompt = \"User: I feel overwhelmed and anxious lately.\\nAssistant:\"\nprint(pipe(prompt)[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T21:53:00.294411Z","iopub.execute_input":"2026-01-30T21:53:00.295200Z","iopub.status.idle":"2026-01-30T21:53:16.736825Z","shell.execute_reply.started":"2026-01-30T21:53:00.295165Z","shell.execute_reply":"2026-01-30T21:53:16.736006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_df.to_csv(\"/kaggle/working/combined_empathy_dataset.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T22:04:25.200078Z","iopub.execute_input":"2026-01-30T22:04:25.200892Z","iopub.status.idle":"2026-01-30T22:04:25.304287Z","shell.execute_reply.started":"2026-01-30T22:04:25.200843Z","shell.execute_reply":"2026-01-30T22:04:25.303689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hf_dataset.to_csv(\"/kaggle/working/combined_hf_dataset.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T22:05:03.737788Z","iopub.execute_input":"2026-01-30T22:05:03.738391Z","iopub.status.idle":"2026-01-30T22:05:04.120967Z","shell.execute_reply.started":"2026-01-30T22:05:03.738361Z","shell.execute_reply":"2026-01-30T22:05:04.120199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}